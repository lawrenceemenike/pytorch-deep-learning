{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcdE0aIyoj4qu6B7QYqQ9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawrenceemenike/pytorch-deep-learning/blob/main/Pytorch_Model_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 09. PyTorch Model Deployment\n",
        "\n",
        "Machine Learning Model Deployment is the act of making your machine learning model available to someoone or something else"
      ],
      "metadata": {
        "id": "lcqhQhooLwg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Get setup"
      ],
      "metadata": {
        "id": "B5OtFwupMNG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting Data\n",
        "\n",
        "The dataset we're going to use for deploying a FoodVision Model"
      ],
      "metadata": {
        "id": "PELpO8CGMTSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated API, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "  import torch\n",
        "  import torchvision\n",
        "  assert int(torch.__version__.split(\".\"[1] >= 12, \"torch version should be 1.12+\"))\n",
        "  assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "  print(f\"torch version: {torch.__version__}\")\n",
        "  print(f\"torchvision version: {torchvision.__version__}\")\n",
        "\n",
        "except:\n",
        "  print(f\"[INFO] torch/torchvision version not as required, installing nightly versions.\")\n",
        "  !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whi/cull3\n",
        "  import torch\n",
        "  import torchvision\n",
        "  print(f\"torch version: {torch.__version__}\")\n",
        "  print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAhKiEBtLzQF",
        "outputId": "6b44f566-21bb-40cf-a6c6-03dd45514e27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision version not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whi/cull3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m478.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "torch version: 2.2.1+cu121\n",
            "torchvision version: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] couldnt find torchinfo.. installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular director, downdload it from Github if it doesnt work\n",
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "  from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "  # Get the going_modular scripts\n",
        "  print(\"[INFO] couldn't find going modular or helper functions scripts.. downloading from Github\")\n",
        "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular\n",
        "  !mv pytorch-deep-learning/helper_function.py\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "  from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "GqH5SwFQNgeZ",
        "outputId": "43be81f3-d2e8-45e9-f07d-97365cd961ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] couldnt find torchinfo.. installing it.\n",
            "[INFO] couldn't find going modular or helper functions scripts.. downloading from Github\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4056, done.\u001b[K\n",
            "remote: Counting objects: 100% (1234/1234), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 4056 (delta 1141), reused 1124 (delta 1124), pack-reused 2822\u001b[K\n",
            "Receiving objects: 100% (4056/4056), 649.94 MiB | 26.28 MiB/s, done.\n",
            "Resolving deltas: 100% (2386/2386), done.\n",
            "Updating files: 100% (248/248), done.\n",
            "mv: missing destination file operand after 'pytorch-deep-learning/going_modular'\n",
            "Try 'mv --help' for more information.\n",
            "mv: missing destination file operand after 'pytorch-deep-learning/helper_function.py'\n",
            "Try 'mv --help' for more information.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'going_modular'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6efdd338f5b0>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mgoing_modular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoing_modular\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_setup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_seeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_loss_curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'going_modular'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6efdd338f5b0>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mv pytorch-deep-learning/helper_function.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -rf pytorch-deep-learning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mgoing_modular\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoing_modular\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_setup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_seeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_loss_curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'going_modular'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting Data\n",
        "\n",
        "The dataset we're going to use for deploying a FoodVision mini model is..\n",
        "Pizza, steaak, sushi 20% dataset (pizza, steak, sushi classes from Food101, rand 20% of samples)\n",
        "\n",
        "We can get data with code from: https://ww.learnpytorch.io/09_pytorch_model_deployment/#1-getting-data"
      ],
      "metadata": {
        "id": "-m2z6a1DSRmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downlaode pizza, steak, sushi images from Github\n",
        "data_20_percent_path = download_data(source = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_susi_20_percent.zip\",\n",
        "                                    destination=\"pizza_steak_sushi_20_percent\")\n",
        "data_20_percent_path"
      ],
      "metadata": {
        "id": "KfiR1yRmRKKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training and test paths\n",
        "train_dir = data_20_percent_path / \"train\"\n",
        "test_dir = data_20_percent_path / \"test\""
      ],
      "metadata": {
        "id": "VXzhrUSvVUmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. FoodVision mini model deployment experiment outline\n",
        "\n",
        "### 3 questions:\n",
        "1. What is my most ideal machine learning model deployment scenario?\n",
        "2. Where is my model going to go?\n",
        "3. How is my model going to function?\n",
        "\n",
        "**FoodVision Mini ideal use case:** A model that perfomrs well and fast\n",
        "\n",
        "* Performs well: 95% accruacy\n",
        "* Fast: as close to real-time as possible (30FPS+)\n",
        "\n",
        "To try and achieve these goals, we're going to build two model experiments:\n",
        "\n",
        "1. EffNetB2 feature extractor (just like in 07. Pytorch Experiment Tracking)\n",
        "2. ViT featuure extractor (just like in 08. Pytoch paper replicatiing)"
      ],
      "metadata": {
        "id": "vVCfSGzpVvpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creating an EffNetB2 feature Extractor\n",
        "\n",
        "Feature Extractor = a term for a transfer learning model that has its base layers frozen and output layers (or head layers) customized to a certain problem.\n",
        "\n",
        "EffNetB2 pretrained model in PyTorch"
      ],
      "metadata": {
        "id": "UCKN0fdEZEC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "torchvision.__version__\n",
        "\n",
        "# 1. Setup pretrained EffNetB2 weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "# 2. Get EffNeB2 transforms\n",
        "effnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "# 3. Setup pretrained model instance\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=\"DEFAULT\")\n",
        "\n",
        "# 4. Freeze the base layers in the model (this will stop all layers from training)\n",
        "for param in effnetb2.parameters():\n",
        "  param.requires_grad=False"
      ],
      "metadata": {
        "id": "cRGcAce0YuOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2.classifier"
      ],
      "metadata": {
        "id": "sEji7xPKaKbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds for reproducibility\n",
        "set_seeds()\n",
        "effnetb2.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True),\n",
        "    nn.Linear(in_features=1408, out_features=1000,bias=True)\n",
        ")"
      ],
      "metadata": {
        "id": "sl7UwtgpbEHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Creating a function to make an EffNetB2 extractor"
      ],
      "metadata": {
        "id": "hfjtDbsvhYVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes:int=3, # defualt output classes = 3(pizza, steak, sushi)\n",
        "                          seed:int=42):\n",
        "\n",
        "# 1, 2, 43 Create EffnetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.model.EfficientNet_B2_Weights.DEFUALT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b3(weigiths=weights)\n",
        "\n",
        "  # 4. Freeze all layers in the base model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5.Change classifier head with random seed for reproducibility\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "NagoGIW3bkWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
        "                                                      seed=42)"
      ],
      "metadata": {
        "id": "kMYXLw0lyzr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.info import summary\n",
        "\n",
        "# Print EffnetB2 model summary (uncomment for full output)\n",
        "summary(effnetb2,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
        "        col_width=20,\n",
        "        row_setting=['var_names'])"
      ],
      "metadata": {
        "id": "bSFeZkJAzrbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Creating DatLaoders for EffNetB2"
      ],
      "metadata": {
        "id": "1J_SEIuI0dLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(tran_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform=effnetb2_transforms,\n",
        "                                                                                                 batch_size=32)"
      ],
      "metadata": {
        "id": "WVT1_Zhk0gph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader_effnetb2), len(test_dataloader_effnetb2), class_names"
      ],
      "metadata": {
        "id": "LmyVeSCk1JrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Training EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "dqs47Dq31PhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# Training function (engine.py)\n",
        "set_seeds()\n",
        "effnet_bw_results = engine.train(model=effnetb2,\n",
        "                                 train_dataloader=train_dataloader_effnetb2,\n",
        "                                 test_dataloader=test_dataloader_effnetb2,\n",
        "                                 epochs=10,\n",
        "                                 optimizer=optimizer,\n",
        "                                 loss_fn=loss_fn,\n",
        "                                 device=device)"
      ],
      "metadata": {
        "id": "OgnJpaPe1UqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_function import plot_loss_curves\n",
        "\n",
        "plot_loss_curve(effnetb2_results)"
      ],
      "metadata": {
        "id": "29PWQ8wv5kZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "9raSimC36V0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8RfFA3Hx51NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# save the model\n",
        "utils.save_model(model=effnetb2,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "ETTs-IMr6ZkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Inspecting the size of the model\n",
        "\n",
        "Why would it be important to consider the size of a saved model?\n",
        "\n",
        "If we are deploy our model to be used on a mobile app/website, there may be limited compute resources. SO if our model file is too large, we may not be able to store/run it on our target device"
      ],
      "metadata": {
        "id": "KCgzQvWF64W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Pathlib\n",
        "\n",
        "# Get the model size in bytes and convert to megabytes\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size / (1024 * 1024)\n",
        "print(f\"Pretrained EffnetB2 feature extractor model size: {round(pretrained_effnetb2_model_size,2)} MB\")\n"
      ],
      "metadata": {
        "id": "34zx45Su63BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Collecting EffNetB2 feature extractoer stats"
      ],
      "metadata": {
        "id": "9u_SsL3dOFj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in EffNetB2\n",
        "effnetb2_total_params = sum(torch.numel(param for param in effnetb2.parameters()))\n",
        "effnetb2_total_params"
      ],
      "metadata": {
        "id": "Ta91ex3MOJgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with EffNetB2 statistics\n",
        "effnetb2_stats = {\"test_loss\": effentb2_results[\"test_loss\"][-1],\n",
        "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
        "                  \"number_of_parameters\": effnetb2_total_params,\n",
        "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "gGrZwz8SO56r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Creating a ViT feature extractor\n",
        "\n",
        "We're up to our second modeling experiment, repeating the steps for EffNetB2 but this time with a ViT feature extractor, see here for ideas"
      ],
      "metadata": {
        "id": "_FCgM02uP4Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check our the ViT heads layer\n",
        "vit = torchvision.models.vit_b_16()\n",
        "vit.heads"
      ],
      "metadata": {
        "id": "_KH8wWKDP2z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_model(num_calsses:int=3,\n",
        "                     seed:int=42):\n",
        "  # Create ViT_B_16 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.VoT_B_16_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "  # Freeze all of the base layers\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Change the classifer heads to suit our needs\n",
        "  torch.manual_seed(seed)\n",
        "  model.heads = nn.Sequential(nn.Linear(in_features=768,\n",
        "                                        out_features=num_classes)\n",
        "  )\n",
        "  return model, transforms\n",
        "\n"
      ],
      "metadata": {
        "id": "vHsdp7W8Q1OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit, vit_transforms = create_vit_model()\n",
        "vit_transforms"
      ],
      "metadata": {
        "id": "i0I4ISXISMug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import Summary\n",
        "\n",
        "# Print ViT model summary (uncomment for full output)\n",
        "summary(vit,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_setting=[\"var_names\"])"
      ],
      "metadata": {
        "id": "5lQ9IJntSbdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Create DataLoaders for ViT feature extractor"
      ],
      "metadata": {
        "id": "j9omM3lDTRBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ViT DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloader(train_dir=train_dir,\n",
        "                                                                                      test_dir=test_dir,\n",
        "                                                                                      transform=vit_transforms,\n",
        "                                                                                      batch_size=32)\n",
        "len(train_dataloader_vit), len(test_dataloader_vit), class_names"
      ],
      "metadata": {
        "id": "j2h7F5N8Tewq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Training ViT Feature Extractor\n",
        "\n",
        "We're up to model experiment number two: a ViT feature extractor"
      ],
      "metadata": {
        "id": "f1WYtuz1UCIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fromn going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train Vit feature extractor with seedfs set for reproducibility\n",
        "set_seeds()\n",
        "vit_results = engine.train(model=vit,\n",
        "                           train_dataloader=train_dataloader_vit,\n",
        "                           test_dataloader=test_dataloader_vit,\n",
        "                           epochs=10,\n",
        "                           optimizer=optimzer,\n",
        "                           loss_fn=loss_fn,\n",
        "                           device=device)"
      ],
      "metadata": {
        "id": "LcQ6lQWZUYHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Plot loss curves of ViT feature extractor"
      ],
      "metadata": {
        "id": "9hssfQ9NVPsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "plot_loss_curves(vit_results)"
      ],
      "metadata": {
        "id": "HpD3ZHAnVTXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Saving Vit feature extractor"
      ],
      "metadata": {
        "id": "s6OPfq7rVtTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=vit,\n",
        "                 targe_dir=\"models\",\n",
        "                 model_name\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_pecent.pth)"
      ],
      "metadata": {
        "id": "9irMlH5kVv2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Check the size of the model"
      ],
      "metadata": {
        "id": "oFx6ds4YV96p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib impor Path\n",
        "\n",
        "# Get the model size in bytes then covnert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feaute_extracotr_pizza_steak_sushi_20_pecent.ptj\").stat().st_size / (1024 * 1024)\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ],
      "metadata": {
        "id": "YwurI5bVWCEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Collecting Vit feature extractor stats"
      ],
      "metadata": {
        "id": "SEdZBHuXWtMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in ViT\n",
        "vit-total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "id": "oq0wfuF1W6pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT Statistics Dictionary\n",
        "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
        "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
        "             \"number_of_parameters\": vit_total_params,\n",
        "             \"model_size (MB)\": pretrained_effnetb2_model_size}"
      ],
      "metadata": {
        "id": "lWNQ10ERXdPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_stats"
      ],
      "metadata": {
        "id": "mljJLG_pYBXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Making predictions with our trained models and timing\n",
        "\n",
        "Our goal:\n",
        "1. performs well (97%+ test accuracy)\n",
        "2. Fast (30+FPS)\n",
        "\n",
        "To test cirtieria two:\n",
        "1. Loop through test images\n",
        "2. Time how long it takes each model to make a prediction\n",
        "\n",
        "Let's work towards making a function called pred_and_store() to do so.\n",
        "\n",
        "First we'll need a list of test image paths"
      ],
      "metadata": {
        "id": "s15SWOrbYrTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get all test data paths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths"
      ],
      "metadata": {
        "id": "aumjnVwLYDRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Creating a function to make across the test dataset\n",
        "\n",
        "1. Create a function taht takes a list of paths and a trained PyTorch and a series of transforms a list of target class names and a target device\n",
        "2. Create an empty list (can return a full list of all predictions later)\n",
        "3. Loop through the target input paths (the rest of the steps will take place inside the loop)\n",
        "4. Create an empty dictionary for each sample (prediciton statistics will go in here).\n",
        "5. Get the sample path and ground truth class from the filepath\n",
        "6. Start the prediction timer\n",
        "7. Open the image using `PIL.Image.open(path)\n",
        "8. Transform the image to be usable with a given model.\n",
        "9. Prepare the model for inference by sending to the target device and turning on `eval()` mode.\n",
        "10. Turn on `torch.inference_mode()` and pass the target transformed image to the model and perform forward pass + calculate pred prob + pred class.\n",
        "11. Add the pred prob + pred class to empty dictionary from step 4.\n",
        "12. End the prediction timer started in step 6 and add the time to the prediction dictionary.\n",
        "13. See if the predicted class matches the ground truth class\n",
        "14. Append the updated precidtion dictionary to the empty list of prediction we created\n",
        "15. Return the list of prediction dictionaries"
      ],
      "metadata": {
        "id": "I6tID4BVZAw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import Pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import defualt timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "# Step 1: Create a function that takes a ist of paths and a trained PyTorch and a series of\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_names: List[str],\n",
        "                   device: str =\"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
        "\n",
        "# Step 2: Create an empty list (can return a full ist of all predictions later).\n",
        "pred_list = []\n",
        "\n",
        "# 3. Loop through the target input paths (the rest of the steps will take place inside the loop).\n",
        "for path in tqdm(paths):\n",
        "\n",
        "  # 4 Create an empty diction for each sampe (prediction statistics will go in here)\n",
        "  pred_dict = {}\n",
        "\n",
        "  # 5. Get the sample path and ground truth class from the filepath.\n",
        "  pred_dict[\"image_path\"] = path\n",
        "  class_name = path.parent.stem\n",
        "  pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "  # 6. Start the prediction timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # 7. Open the image using `PIL.Image.open(path)`\n",
        "  img = Image.open(path)\n",
        "\n",
        "  # 8. Transform the image  to a usable with a given model\n",
        "  transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "  # 9. Prepare the model for inference by sending to the target device\n",
        "  model = model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  # 10. Turn on `torch.inference_mode(`) and pass the target transformed image to the\n",
        "  with torch.inference_mode():\n",
        "    pred_logit = model(transformed_image)\n",
        "    pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
        "    pred_label = torch.argmax(pred_pron, dim=1) # turn prediciton proability into prol\n",
        "    pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on\n",
        "\n",
        "  # 11. Add the pred prob + pred class to empy dictionary from step 4.\n",
        "  pred_dict[\"pred_prob\"] = roind(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "  pred_dict[\"pred_calss\"] = pred_class\n",
        "\n",
        "  # 12. End the prediction timer started in step 6 and add the time to the prediciton\n",
        "  end_time = timer()\n",
        "  pred_dict[\"time_for_ored\"] = round(end_time-start_time, 4)\n",
        "\n",
        "  # 13. See if the predicted class matches the ground truth class\n",
        "  pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "  # 14. Append the updated prediction dictionary to the empty list of predictions we ahve\n",
        "  pred_list.append(pred_dict)\n",
        "\n",
        "  # 15. Return the list of prediciton dictionaries\n",
        " return pred_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "UmycgTnjYDy5",
        "outputId": "430c4b0d-f65b-4f4b-9d22-ed53a8d880ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-5-db95b222c596>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-db95b222c596>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Our goal:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make predictions and timing predictions with EffNetB2\n",
        "\n",
        "Let's test our `pred_and_store()` function.\n",
        "\n",
        "Two things to note:\n",
        "1. Device - we're going to hardcode our predictions to happen on CPU (becuase you wont always be sure of having a GPU when you deploy the model)\n",
        "2. Transforms - we want to make sure each of the models are predicitng on images that have been prepared with a"
      ],
      "metadata": {
        "id": "MippNiH5wflj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions test dataset with EffNetB2\n",
        "\n",
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                          model=effnetb2,\n",
        "                                          transform=effnetb2_transforms,\n",
        "                                          class_names=class_names,\n",
        "                                          device=\"cpu\") #hardcode predictions to happen"
      ],
      "metadata": {
        "id": "P5YSlil-hS6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
        "effnetb2_test_perd_df.head()"
      ],
      "metadata": {
        "id": "pfqKfC3myCYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of correct predictions\n",
        "effnetb2_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "B-ANCbD-yWiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average time per prediciton\n",
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred}\")"
      ],
      "metadata": {
        "id": "stxZvFZ3yoHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Making and timing predictions with ViT\n"
      ],
      "metadata": {
        "id": "dgioluym0zWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make list of prediction dictionaries with VoT feature extractor model on test images\n",
        "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                     model=vit,\n",
        "                                     transform=vit_transforms,\n",
        "                                     class_names=class_names,\n",
        "                                     device='cpu')"
      ],
      "metadata": {
        "id": "_HKzlbt4YIoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first couple of ViT predictions\n",
        "vit_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "xNlQY7V31arP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn vit_test_pred_dicts\n",
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
        "vit_test_pred_df.head()"
      ],
      "metadata": {
        "id": "HBgA-PIH1fg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see how mmany correct\n",
        "vit_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "eKRAzPv91-7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average time per prediction for ViT model\n",
        "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"Vit average timer per prediction: {vit_average_time_per_pred}\")"
      ],
      "metadata": {
        "id": "jKMj0EL42jAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add average time per predictiion to ViT stats\n",
        "vit_stats[\"time_per_pred_gpu\"] = effnetb2_average_time_per_pred"
      ],
      "metadata": {
        "id": "fXuLRpB52wO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Comparing model results, prediction times and size"
      ],
      "metadata": {
        "id": "y6wBSvPD3J-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tyrn stat dictionaries into DataFrames\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "# Add coulmn for model names\n",
        "df[\"model\"] = [\"EffNeB2\", \"Vit\"]\n",
        "\n",
        "# Convert accuracy to percentagtes\n",
        "df[\"test-acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "ZTGmkMqi3O84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "which model is better?\n",
        "* test_loss (lower is btter) - ViT\n",
        "* test_acc (higher is better) - ViT\n",
        "* number_of_parameters (generally lower is better*) - if a model ahs more parameters, it generally takes linger to compute"
      ],
      "metadata": {
        "id": "mxo9dZtW3yhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ViGT to EffNetB2 across different characteristics\n",
        "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]),\n",
        "            columns=[\"Vit to EffnetB2 ratios\"]).T"
      ],
      "metadata": {
        "id": "H8vaUMMjYHBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Visualising the speed vs. performance tradeoff\n",
        "\n",
        "So we've compared our EffNetB2 and ViT feature extractor models, now let;s visualize the comparison with a speed vs. performance plot.\n",
        "\n",
        "We can also do so with maptlotlib:\n",
        "1. Create a scatter plot from the comparison DataFrame to compare EffNetB2 and ViT across test accuracy and prediction time.\n",
        "2. Add titles and labels to make our plot look nice\n",
        "3. Annotate the sample on the scatter plot so we know what's going on\n",
        "4. Create a legend based on the model sizes (model_size (MB)."
      ],
      "metadata": {
        "id": "p1i6TJ0NZiCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a plot from model comparison DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(data=df,\n",
        "                     x=\"time_per_pred_cpu\",\n",
        "                     y=\"test_acc\",\n",
        "                     c=[\"blue\", \"orange\"],\n",
        "                     s=\"model_size (MB)\")\n",
        "\n",
        "# 2. Add titles and labels to make our plot lookg good\n",
        "ax.set_title('Food Vision Mini Inference Speed vs Performance', fontsize=18)\n",
        "ax.set_xlable(\"Prediction time per image (seconds)\", fontsize=14)\n",
        "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
        "ax.tick_params(axis=\"both\", labelsize=12)\n",
        "ax.grid(True)\n",
        "\n",
        "# 3. Annotate the sample on the scatter plot so we know what's going on.\n",
        "for index row in df.iterrows():\n",
        "  ax.annotate(text=row['model'],\n",
        "              xq=row([\"time_per_perd_cpu\"]+0.006, row[\"test_acc\"]+0.006),\n",
        "              size=12)\n",
        "\n",
        "# 4. Create a legend based on the model sizes (model_size (MB)).\n",
        "handles, labels = scatter.legned_elements(prop=\"sizes\", alpha=0.5)\n",
        "model_size_legend = ax.legnd(jandles,\n",
        "                             lables,\n",
        "                             loc='lower right',\n",
        "                             title=\"Model size (MB)\")\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "okbfgtyqtSB9",
        "outputId": "066032ad-0292-4b21-f0b1-3e12f5c1e98b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-df6b086e09e4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m scatter = ax.scatter(data=df,\n\u001b[0m\u001b[1;32m      5\u001b[0m                      \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"time_per_pred_cpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_acc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Bringing Food vision mini to live by creating a Gradio demo\n",
        "\n",
        "We've chosing to deploy EffNetB2 as it fulfuls our criteria the best.\n",
        "\n",
        "What is Gradio?\n",
        "\n",
        "Gradio is the fastest way to demo your machine learning model with a friednly interface"
      ],
      "metadata": {
        "id": "NGIghaTOwIte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Gradio Overview\n",
        "\n",
        "Gradio helps you create machine learning demos\n",
        "Why create a demo?\n",
        "\n",
        "so other pwople can try our models and we can test them in the real-world\n",
        "Deployent is as important as training. The overal premise of Graido is to map inputs -> functionmodel .> outputs"
      ],
      "metadata": {
        "id": "17DVc8Pd2e_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Creating a function to map our inputs and outputs"
      ],
      "metadata": {
        "id": "eoMbfqgb4emD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put our model on the CPU\n",
        "effnetb2 = effnetb2.to(\"cpu\")\n",
        "\n",
        "# check the device\n",
        "next(iter(effnetb2.parameters())).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "ft3pk3lnuLjR",
        "outputId": "68523d15-fd68-43ca-b761-6ac9c60ee6d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'effnetb2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-be55f4f8f933>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Put our model on the CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meffnetb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meffnetb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# check the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meffnetb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'effnetb2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a function called `predict()` to go from:\n",
        "\n",
        "```\n",
        "images of food -> ML model (Effnetb2) -> outputs (food class label)"
      ],
      "metadata": {
        "id": "5nYs16KM4nnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-ZJCijZ04vZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, Dict\n",
        "\n",
        "def predict(img) -> Tuple[Dict, Float]:\n",
        "  # Start a timer\n",
        "  start_timer = start()\n",
        "\n",
        "  # Transform the input image for use with EffNetB2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
        "\n",
        "  # Put model into eval mode, make prediction\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # pass transformed image through the model and turn the prediction logits into probabilities\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary\n",
        "  pred_labels_and_probs = class_names[i]: float(pred_probs[0][i] for i in range(len(class_names)))\n",
        "\n",
        "  # Calculate pred time\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  # Return pred dict and pred time\n",
        "  return pred_labels_and_probs, pred_time"
      ],
      "metadata": {
        "id": "zCb4ph7D4ulh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Get a list of all test image filepaths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[0]\n",
        "\n",
        "# Randomly select a tet image path\n",
        "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
        "random_image_path\n",
        "\n",
        "# Open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predicting on image at path: \" {random_image_path}\\n)\n",
        "\n",
        "# Predict on the target image and print out the outputs\n",
        "pred_dict, pred_time = predict(img=image)\n",
        "print(pred_dict)\n",
        "print(pred_time)\n",
        "\n"
      ],
      "metadata": {
        "id": "mYfBUwXDsh4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Creating a list of example images"
      ],
      "metadata": {
        "id": "bMF7xy9Mt_a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of example inputs to our Gradio demo\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
        "example_list"
      ],
      "metadata": {
        "id": "iT1xH59Dtd7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Build a Gradio interface\n",
        "\n",
        "Les use `gr.Interface()` to go from:\n",
        "```\n",
        "input: image -> transform -> predict with EffNetB2 -> output: pred, prob prob, time\n",
        "```"
      ],
      "metadata": {
        "id": "i8_55Xz7un9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article\n",
        "title = \"FoodVision Mini\"\n",
        "description = \"An EfficinetNetB2 featuer extactor computer vision model to classify pizza, steak or sushi\"\n",
        "article = \"created at 09. PyTorch Model Deployment\"\n",
        "\n",
        "# create the gradio demo\n",
        "demo = gr.Interface(fn=predict, # maps inputs to outputs\n",
        "                    inputs=gr.Image(type=\"pil\"),\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predicitons\"),\n",
        "                             gr.Number(lanel=\"Prediction time (s)\")],\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "# Launch the Demo\n",
        "demo.launch(debug=False,\n",
        "            share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "ns4aaR0pstmP",
        "outputId": "96d00eb8-9483-4002-d01d-40c8c3e5263f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'predict' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e4ad9c817fe4>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# create the gradio demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m demo = gr.Interface(fn=predict, # maps inputs to outputs\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pil\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     outputs=[gr.Label(num_top_classes=3, label=\"Predicitons\"),\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Turning our FoodVision Mini Gradio Demo into a deployable app\n",
        "\n",
        "Our Gradio demos from Google Colab are fantastic but expires within 72 hours/\n",
        "\n",
        "To fix this, we're going to prepare our app files so we can host them on Hugging Face Spaces"
      ],
      "metadata": {
        "id": "BVnYAxYnylT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 What is Hugging Face Spaces?\n",
        "\n",
        "Hugging Face Spaces offer a simple way to host ML demo apps directly on your profile or your organisation's profile. This allows you to create your ML portfolio, showcase your projects at conferences or to stakeholders, and work collaboratively with other people in the ML ecosystem."
      ],
      "metadata": {
        "id": "WOISGc6Ly7Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Deployed Gradio App Structure:\n",
        "\n",
        "Let's start to put all of our app file into a signle directory:\n",
        "\n",
        "```\n",
        "Colab -> folder with all Gradio files -. upload app files to Hugging Face Spaces -> Deploy"
      ],
      "metadata": {
        "id": "HcI39vfhz5RV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Create a `demos` folder to store our Food vision app files"
      ],
      "metadata": {
        "id": "2ZHOhvF74bOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision mini demo path\n",
        "foodvision_min_demo_path = Path(\"demos/foodvision_min/\")\n",
        "\n",
        "# Remove files taht might exist and create a new directory\n",
        "if foodvision_min_demo_path.exists():\n",
        "  shutil.rmtree(foodvision_mini_demo_path)\n",
        "  foodvision_min_demo_path.mkdir(parents=True,\n",
        "                                 exist_ok=True)\n",
        "else:\n",
        "  foodvision_min_demo_path.mkdir(parents=True,\n",
        "                                 exist_ok=True)\n",
        "\n",
        "!ls demo/foodvision_mini/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yAQRDA3xChE",
        "outputId": "56b6e690-8d26-480e-b992-22c81d4ee019"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.24.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.14.0 (from gradio)\n",
            "  Downloading gradio_client-0.14.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.3.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.14.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.14.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=031d9af811f1609275b0c230887e0ffea2aba8f6e21ea4aefa1e9dead4869a57\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.0 ffmpy-0.3.2 gradio-4.24.0 gradio-client-0.14.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.4 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Creating a folder of example images to use with our FoodVision MIni Demo\n",
        "\n",
        "What we want:\n",
        "> 3 Images in `examples/` directory\n",
        "> Images should be from the test set"
      ],
      "metadata": {
        "id": "osLMOzyA6FmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create an example directory\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
        "foodvision_mini_examples_path.mkdir(parents=True, exist_pk=True)\n",
        "\n",
        "# Collwct three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_pecent/test/sushi/5927799.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/steak/362237.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
        "\n",
        "# Copy the three images to the examples directory\n",
        "for example in foodvision_mini_examples:\n",
        "  destination = foodvision_mini_examples_path / example.name\n",
        "  shutil.copy2(src=example,\n",
        "               dst=destination)\n"
      ],
      "metadata": {
        "id": "XxsS8jKQ4lYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now verify that we can get a list of lists from our `examples/` directory"
      ],
      "metadata": {
        "id": "zhthDq6O8Jqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get example filepaths in a list of lists\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
        "example_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "-AqNea1F8PAD",
        "outputId": "127b8f0a-3f51-4a79-aa75-48f904ce96e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'foodvision_mini_examples_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5ddc0044aa44>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get example filepaths in a list of lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexample_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"examples/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoodvision_mini_examples_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mexample_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'foodvision_mini_examples_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Moving our trained EffNetB2 model to our FoodVision Mini Demo directory\n",
        "\n"
      ],
      "metadata": {
        "id": "BmofebYC8xCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a source path for our target model\n",
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained-effentb2_feature\"\n",
        "\n",
        "# Create a destination path for our target model\n",
        "effenetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\n",
        "# Try to move the model file\n",
        "try:\n",
        "  print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
        "\n",
        "  # move the model\n",
        "  shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
        "              dst=effnetb2_foodvision_mini_model_destiantion)\n",
        "  print(f\"[Info] model move complete\")\n",
        "# if the model has already been moved, check if it exists\n",
        "except:\n",
        "  print(f\"[INFO] model found at {effenetb2_foodvision_mini_mode_path}, perharps its already been moved\")\n",
        "  print(f\"[INF)] model exists at {effnetb2_foodvison_mini_model_destiantion}: {effentb2_foodvison_mini_models_destiantion.exist()}\")"
      ],
      "metadata": {
        "id": "PcxfLlvj8oFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Turning off EffNetB2 model into a Python script (`model.py`)\n",
        "We have a saved `.pth`  model `state_dict` and want to load it into a model instance\n",
        "\n",
        "Let's  move our `create_effnetb2_model()` function to a script so we can resuse"
      ],
      "metadata": {
        "id": "9yAxikNJ_bEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos.foodvision_mini/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def create_effnetb2_model(num_classses:int=3,\n",
        "                          seed:int=42):\n",
        "  # 1, 2, 3 Create EffnetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.EfficeinetNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all layers in the base model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "    # 5. Change classifier head with random seed for reproducibility'\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifer=ier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_features)\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "7teoITPl_vAo",
        "outputId": "20fdf39e-6d7f-468e-ad6f-4d303df86fe2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos.foodvision_mini/model.py\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'demos.foodvision_mini/model.py'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-47b7c15d6b9b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'demos.foodvision_mini/model.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"import torch\\nimport torchvision\\n\\ndef create_effnetb2_model(num_classses:int=3,\\n                          seed:int=42):\\n  # 1, 2, 3 Create EffnetB2 pretrained weights, transforms and model\\n  weights = torchvision.models.EfficeinetNet_B2_Weights.DEFAULT\\n  transforms = weights.transforms()\\n  model = torchvision.models.efficientnet_b2(weights=weights)\\n\\n  # 4. Freeze all layers in the base model\\n  for param in model.parameters():\\n    param.requires_grad = False\\n\\n    # 5. Change classifier head with random seed for reproducibility'\\n    torch.manual_seed(seed)\\n    model.classifer=ier = nn.Sequential(\\n        nn.Dropout(p=0.3, inplace=True),\\n        nn.Linear(in_features=1408, out_features=num_features)\\n    )\\n\\n    return model, transforms\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-98>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'demos.foodvision_mini/model.py'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from demos.foodvision_mini import model\n",
        "effnetb2_model, effnetb2_transforms_import = model.create_effnetb3_model()"
      ],
      "metadata": {
        "id": "PfPuHSLvA5ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.7 Truning our FoodVision Mini Gradio app into a Python script (`app.py`)\n",
        "\n",
        "The app.py file will have four major parts:\n",
        "1. Imports and calss names\n",
        "2. Model and transforms preparation\n",
        "3. Predict function\n",
        "4. Gradio app - our gradio interface + lauch commnad"
      ],
      "metadata": {
        "id": "RaBbM833Bf74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.8 creating `requirements.txt` file\n",
        "1. torch\n",
        "2. torchvision\n",
        "3. Gradio"
      ],
      "metadata": {
        "id": "GuLiiqioJz8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_min/app.py\n",
        "### 1. Imports and class names stetup ###\n",
        "import fradio as gr\n",
        "import os\n",
        "import torhc\n",
        "\n",
        "from model import creatae_effnetb2_model\n",
        "from timeit import default as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "class_names = ['pizza', 'steak', 'sushi']\n",
        "\n",
        "### 2. Model and transforms preparation\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=3\n",
        ")\n",
        "\n",
        "# load save wieghts\n",
        "effnetb2_load_stae_dict(\n",
        "    torch.load(\n",
        "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "        map_location=torch.device(\"cpu\")\n",
        "    )\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Tras=nsform the input image for use with EffnetB2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
        "\n",
        "  # Put model into eval mode, make prediction\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the predicition logits into orobabilites\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i] for i in range(len(class_names)))}\n",
        "\n",
        "    # Calculate pred time\n",
        "    end_time = timer()\n",
        "    pred_time = round(end_time - start_time, r)\n",
        "\n",
        "    # Return pred dict and pred time\n",
        "    return pred_labels_and_probs, pred_time\n",
        "\n",
        "  ### 4. Gradio appp ###\n",
        "  # Create title, description and article\n",
        "  title = \"FoodVision Mini\"\n",
        "  description = \"An [EfficientNetB2 feature extractor]\"\n",
        "  article = \"Created at [09/ PyTorch Model Deployment\"]\n",
        "\n",
        "  # Create example list\n",
        "  example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "  # Create the Gradio demo\n",
        "  demo = gr.Interface(fn=predict,\n",
        "                      inputs=gr.Image(type=\"pil\")\n",
        "                      Ouputs=[gr.Label(num-top_calsses=3, Label=\"Predictions\"),\n",
        "                              gr.Number(label=\"Prediction time (s)\")],\n",
        "                      examples=example_list,\n",
        "                      title=title,\n",
        "                      description=description,\n",
        "                      article=article)\n",
        "  # Launch the demo\n",
        "  demo.launch(debug=False,\n",
        "              share=True)\n"
      ],
      "metadata": {
        "id": "B3_V-Cf-BpKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.8 Creating a requiremetns file for Foodv=Vision mini (`requirements.txt`)\n",
        "\n",
        "The requiremetns file will tell our Huging Face Space whatr software dependencies our app requires.\n",
        "\n",
        "1. torch\n",
        "2. torchvision\n",
        "3. Gradio"
      ],
      "metadata": {
        "id": "yZU2xEZcKEjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_min/requirements.txt\n",
        "\n",
        "torch==1.12.1\n",
        "torchvision==0.13.0\n",
        "gradio==3.1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "ezjuzBZPKQ3G",
        "outputId": "07379003-a4ff-49c8-f416-1bcf8b3a47a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_min/requirements.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'demos/foodvision_min/requirements.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7e2eb39f10ec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'demos/foodvision_min/requirements.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-98>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'demos/foodvision_min/requirements.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9 Deploying our Food Vision Model to Hugging Face Spaces"
      ],
      "metadata": {
        "id": "0kGsSsuNLCW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 DOwnloading our FoodVision Mini app files\n",
        "\n",
        "We want to download oru `foodvision_mini` demo app so we can upload it to hugging face spaces"
      ],
      "metadata": {
        "id": "k_sXCK4lVbz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/ffodvision_mini/examples"
      ],
      "metadata": {
        "id": "VvW8vfZWKant"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change into the foodvision_mini directory and then zip it from the inside\n",
        "!cs demos.foodvision_min && zip -r ../foodvision_mini.zip"
      ],
      "metadata": {
        "id": "ewMHug5zVu-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into the foodvision_mini directory and then zip it from the inside\n",
        "!cs demos.foodvision_min && zip -r ../foodvision_mini.zip * -x \".pyc\" \"*.ipynb\" \"\"*"
      ],
      "metadata": {
        "id": "hZ7oubGwWICu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downliad\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.downlaod(\"demos/foodvision_mini.zip\")\n",
        "except:\n",
        "  print(f\"Not running in google colab, cant use  google.colab.files.download(0, please download foodvision.zip manually)\")"
      ],
      "metadata": {
        "id": "sDV06HPNXHWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Running Gradio app locally"
      ],
      "metadata": {
        "id": "hNPVPhPAXzqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IPython is a library to help make Python interacgtive\n",
        "from IPython.display import IFrame\n",
        "\n",
        "# Embed FoodVision Mini Gradio demo\n",
        "IFrame(src=\"https://hf.spaces/embed/mrdbourke/foodvision_mini/+\", width=900, height=750)"
      ],
      "metadata": {
        "id": "vF6x82gFX2kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Creating FoodVision Big!!!\n",
        "\n",
        "FoodVision Mini WOkrs well with 3 classes (pizza, steak, sushi)\n",
        "\n",
        "Lets step things up a notch and make FoodVision BIG! using all of the Food101 classes."
      ],
      "metadata": {
        "id": "CW_q90Y9KNed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Creating a model for FoodVsion Big + transforms\n"
      ],
      "metadata": {
        "id": "n3nSjKg6KnnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 model and transforms\n",
        "\n",
        "effnetb2101, effenetb2_transfoms = create_effnetb2_model(num_classes=101)"
      ],
      "metadata": {
        "id": "JOoRkJNGKbid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# print EffNetB2 model summary\n",
        "summary(effnetb2_food101,\n",
        "        input_size=91, ,3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]"
      ],
      "metadata": {
        "id": "ciCXgTEsLKZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are working with a larger dataset, we amy want to introduce some data augementation techniques. This is because with larger model, overfitting becomes a problem\n",
        "* Because we're are using a large number of classes, let's use Trivaial Augment as our data augementation technique\n",
        "\n",
        "for a list of state-of-the-art computer vision"
      ],
      "metadata": {
        "id": "CX9ivk_xMQ8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food_train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.TrivialAugmentWide(),\n",
        "    effnetb2_transforms\n",
        "])"
      ],
      "metadata": {
        "id": "HIICBIyKLvYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Getting data for FoodVision Big"
      ],
      "metadata": {
        "id": "RwbPTTtqNHXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviion import datasets\n",
        "\n",
        "# Setup data directory\n",
        "from pathlib import Path\n",
        "data-dir = Path(\"data\")\n",
        "\n",
        "# Get the training data (~750 X 101 classes)\n",
        "train_data = datasets.Food101(root=data_dor,\n",
        "                              split=\"train\",\n",
        "                              transform=food101_train_transforms,\n",
        "                              download=True)\n",
        "\n",
        "# Get the testing data (~250 images x 101 classes)\n",
        "test-data = datasets.Food101(root=data_dir,\n",
        "                             split=\"test\",\n",
        "                             transform=effnetb2_transforms,\n",
        "                             download=True)"
      ],
      "metadata": {
        "id": "bxh4FEM9NBQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Food101 class names\n",
        "food101_class_names = train_data.classes\n",
        "\n",
        "# View the first 10\n",
        "food101_calls:names[:10]"
      ],
      "metadata": {
        "id": "nFRkio5iNBS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Creatign a subset of the Food 101 dataset for faster experimenting\n",
        "\n",
        "Why create a subset?\n",
        "\n",
        "We want our first few experiments to run as quick as possible\n",
        "\n",
        "We know foodvision mini works prety well but this is the first time we've upgraded to 101 classes.\n",
        "\n",
        "To do so ;ete's maek a susbset of 20%\n"
      ],
      "metadata": {
        "id": "hvgF1_UIOoBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import random_split\n",
        "\n",
        "def split_dataset(dataeset:torchvision.datasets,\n",
        "                  split_size:float=0.2,\n",
        "                  seed:int=42):\n",
        "  # Create split lengths based on original dataset length\n",
        "  length_1 = int(len(dataset)) * split_size\n",
        "  length_2 = lent(dataset) - length_l1 # remaining length\n",
        "\n",
        "  # Print out info\n",
        "  print(f\"[INFO]Splitting dataset of length {len(dataset)} into splits of size: {length_1} and {length_2}\")\n",
        "\n",
        "  # Create splits with given random seed\n",
        "  random_split_1, random_split_2 = torch.utils.data.random_split(dataset,\n",
        "                                                                 lengths[lenght_1, length_2],\n",
        "                                                                 generator=torch.manual_seed(seed))\n",
        "\n",
        " return random_split_1, random_split_2"
      ],
      "metadata": {
        "id": "YsVQBo6QNBVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training 20^ split Food101\n",
        "train_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n",
        "                                              split_size=0.2)\n",
        "\n",
        "# Create testing 20% split Food101\n",
        "test_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n",
        "                                                split_size=0.2)"
      ],
      "metadata": {
        "id": "YOgQcGpYNBYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data_food101_20_percent), len(test_data_food101_20_percent)"
      ],
      "metadata": {
        "id": "kmwYdOS6Rtx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4 Turning FoodVision101 to DataLoaders"
      ],
      "metadata": {
        "id": "eblN07rlR189"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "nUM_WORKERS = 2\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# create Food101 20% training DataLoader\n",
        "train_dataloader_food101_20_percent = torch.utils.data.DataLoader(dataaset=train_data_food101_20_percent,\n",
        "                                                                  batch_size=BATCG_SIZE,\n",
        "                                                                  num_workers=NUM_WORKERS)\n",
        "\n",
        "# Create Food101 20%  testing DataLoader\n",
        "test_dataloader_food101_20-percent = torch.utils.data.DataLoader(dataset=test_data_food101_20_percent,\n",
        "                                                                 batch_size = BATCH_SIXE,\n",
        "                                                                 shuffle=False,\n",
        "                                                                 num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "fU3mD2W9NBbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_datalaoder_food101_20_percent), len(test_datalaoder_food101_20_percent)"
      ],
      "metadata": {
        "id": "o_U-jyfcTIT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5 Training FoodVision Big!!!\n",
        "\n",
        "Things for training:\n",
        "* 5 epochs\n",
        "* Optimizer: `torch.optim.Adam(lr=1e-3)`\n",
        "* Loss function: `torch.nn.CrossEntropyLoss(label_smoothing=0.1)`\n",
        "\n",
        "Why use label smoothing?\n",
        "\n",
        "Label smoothing helps to prevent overfitting (Its a regularization technique).\n",
        "\n",
        "Without label smoothing and 5 classes:\n",
        "\n",
        "wit"
      ],
      "metadata": {
        "id": "LfPmttiwTRwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optiimizer\n",
        "optimizer = torch.optim.Adma(params=effentb2_food101.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# Setup loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# Want to beat the origianl Food101 paper's result of 56.4% accuracy on the test dataset with 20% of the data\n",
        "set_seeds()\n",
        "effnetb2_food101_results = engine.train(model=effnetb2_food101,\n",
        "                                        train_dataloader=train_dataloader_food101_20_percent,\n",
        "                                        test_dataloader=test_dataloader_food101_20_percent,'\n",
        "                                        optimizer=optimizer,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        epochs=5,\n",
        "                                        device=device)"
      ],
      "metadata": {
        "id": "ksaI0K2YTIWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.6 Inspecting loss curves for FoodVision Big model\n"
      ],
      "metadata": {
        "id": "QtsHGZDvVq52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "plot_loss_curves(effnetb2_food101_results)"
      ],
      "metadata": {
        "id": "sPsbyDtjTIZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7 Save and load FoodVision Big model\n",
        "\n"
      ],
      "metadata": {
        "id": "OWIBGpvRV0It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gong_modular.going_modular import utils\n",
        "\n",
        "# Create a model path\n",
        "effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extactor_food101_20_percent.pth\"\n",
        "\n",
        "# Save FoodVision Big Model\n",
        "utils.save_model(model=effnetb2_food101,\n",
        "                 target_dir='models/',\n",
        "                 model_name=effnetb2_food101_model_path)"
      ],
      "metadata": {
        "id": "YjmlynvZVzDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 compatible Effnetb2 instance\n",
        "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
        "\n",
        "# load the saved model's state dict()\n",
        "loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extracotr_fodd101_20_percent.pth\"))"
      ],
      "metadata": {
        "id": "35quNAEgWUXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.8 Checking FoodVision Big model size"
      ],
      "metadata": {
        "id": "yeGfbhY6XuC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes and then convert to megabytes\n",
        "pretrained_effnetb2_food101_model_size =Path(\"models\", effnetb2_food101_model_path).stat().st.size // (1024 * 1024) # division converts\n",
        "print(f\"Pretrained EffnetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")"
      ],
      "metadata": {
        "id": "diP75XNGXtQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Turning FoodVision Big to a deployable app\n",
        "Deploying a modell allwos you to see how your model goes in the"
      ],
      "metadata": {
        "id": "b7i5M0FrX38X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create Foodvision Big demo path\n",
        "foodvision_big_demo_path = path(\"demos/fodovion_big/\")\n",
        "\n",
        "# Make FoodVision Big demo directory\n",
        "Foodvision_big_demo_path.mkdir(parents=True,\n",
        "                               exist_ok=True)\n",
        "\n",
        "# Make FoodVision Big demo examples directory\n",
        "(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exisit_ok=True)\n"
      ],
      "metadata": {
        "id": "msGJb_YJZr_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1 Downloading an example image and moving it to the example directory"
      ],
      "metadata": {
        "id": "G4nEcXoHafQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and move example image\n",
        "!wget https://github.com/mrdbourke/pytorch-deep-learning/raw/\n",
        "!mv 04-pizza-dad.jpeg demos/foodvision_bg/examples/04-pizza-dad.jpeg"
      ],
      "metadata": {
        "id": "XkWezEgTajhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2 Savign Food101 class names to file (`class_names`)"
      ],
      "metadata": {
        "id": "DsMwn-f_bxzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the first Food101 calss names\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "id": "8bysIuyRb9SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create path to Food101 class names\n",
        "foodvision_big_class_names_oath = foodvision_big_demo_path / \"class_names.txt\"\n",
        "foodvision_big_class_names_path"
      ],
      "metadata": {
        "id": "lnQrldp8cDoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write food101 class names to text file\n",
        "with open(foodvision_big_class_names_path, \"w\") as f:\n",
        "  print(f\"[INFO] saving Food101 class names to {foodvision_big_class_names_path}\")\n",
        "  f.write(\"\\n\".join(food101_class_names)) # new line per class names"
      ],
      "metadata": {
        "id": "h9Hr7AhPcDw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Food101 class names and seve them to a list\n",
        "with open(foodvision_big_class_names_path, \"r\") as f:\n",
        "  food101_class_names_loaded = [food for food in f.readlines()]\n",
        "food101_class_names_loaded"
      ],
      "metadata": {
        "id": "c9_a6lKcctHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3 Truning our FoodVision big model into a python script (`model.py`)"
      ],
      "metadata": {
        "id": "ChigUoN6dped"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
        "  weights = torcgvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all layers in the base model'\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5. Change classifier head with random seed for reporducitbility\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "K4nl_3lLcy00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4 Turning our FoodVision Big Gradio app into a python script (`app.py`)\n",
        "\n",
        "1. Imports and class names setup - for class names, we'll need to import from `class_names.txt` rather than with a Python list\n",
        "2. Model and transforms preparaation - we'll need to make sure our model is suitbale for Foodvision Big\n",
        "3. Predict function (predict()) - this can stay the same as the original\n",
        "4. Gradip app - our Gradio app interface + launch comman - this will change slightly from foodvision mini to reflect the food vision"
      ],
      "metadata": {
        "id": "YYoyqzuffUx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "writefile demos/foodvision_big/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timit import default_timer as timer\n",
        "from typeing timport Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "with open(\"class_names.txt\", \"r\") as f:\n",
        "  class_names = [food_name.strip() for food_name in f.readlines()]\n",
        "\n",
        "  ### 2. Model and transforms preparations ###\n",
        "  effnetb2, effnet2_transforms = create_effnetb2_model(num_classes=101)\n",
        "\n",
        "  # Load svaed weights\n",
        "  effnetb2.load_state_dict(\n",
        "      torch.load(f=\"\")\n",
        "  )\n",
        "\n",
        "  def\n",
        "\n"
      ],
      "metadata": {
        "id": "QVtxlYMZctKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BhUEMiXXctNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}